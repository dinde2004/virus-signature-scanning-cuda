---
title: "CS3210 - Assignment 2"
author: "Doan Quoc Thinh (e0960479), Nguyen Cao Duy (e0969050)"
date: "2024-10-21"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, include = TRUE,
                      fig.align = "center",  out.width = "80%", echo = FALSE)
library(tidyverse)
library(patchwork)
```

## Performance Optimization

### Optimization 1: Organizing sample and signature data in a contiguous block

`kernel_without_opt1.cu` allocated memory and transferred data from host to device for each sample and signature separately. The first drawback is that this resulted in multiple `cudaMalloc` and `cudaMemcpy` calls as the number of samples and signatures increased. This will create more overhead as the GPU had to allocate many small chunks of memory and transfer data multiple times. The second drawback is that the data was not contiguous in memory on device. Based on our parallelization strategy, each thread accesses a pair of sample and signature data. If the data is not contiguous, each thread in a warp will access different memory locations, leading to scattered and non-coalesced memory accesses which can reduce performance.

To optimize this, `kernel_skeleton.cu` first concatenated all sample sequences, sample qualities, and signature sequences into 3 contiguous strings on the host before allocating and transferring them to the device. In doing so, we need to add an extra step of calculating additional offset arrays and pass them to the kernel to access the correct data for each thread. However, the gain in performance from coalesced memory accesses outweighs the overhead of calculating the offsets.

### Optimization 2: Swapping sample and signature access pattern

`kernel_without_opt2.cu` coordinate the threads to share the same signature (`signature_idx` remains the same) but access different samples (`sample_idx` varies):

```cpp
int signature_idx = idx / num_samples;
int sample_idx = idx % num_samples;
```

and `kernel_skeleton.cu` optimized it by coordinating the threads to share the same sample (`sample_idx` remains the same) but access different signatures (`signature_idx` varies):

```cpp
int sample_idx = idx / num_signatures;
int signature_idx = idx % num_signatures;
```

This optimization works due to the length constraint of the samples and signatures being different (samples are at least **10 times** longer than signatures). Compared to the unoptimized code, our optimized code has a better utilization of cache as threads in a warp access the same memory location more often (sharing the same sample sequence, which is much longer than a signature) while access the different memory location (different signature sequences, which are much shorter than different sample sequences) less often.

### Performance Comparison

Based on the graph below, we can see that the optimized versions of the kernel outperformed the unoptimized versions and the benchmark on both `a100-40` and `h100-96` GPUs. Optimization 2 has a significant improvement to our implementation. Details of the tests used can be found in the appendix.

```{r}
data_1 <- read_csv("report_data/optimization/a100-40.csv") %>% 
  pivot_longer(cols = -Test, names_to = "Version", values_to = "Time")
data_2 <- read_csv("report_data/optimization/h100-96.csv") %>% 
  pivot_longer(cols = -Test, names_to = "Version", values_to = "Time")

# Create the box plot
plot_1 <- ggplot(data_1, aes(x = Version, y = Time)) +
  geom_boxplot() +
  labs(title = "A100-40", x = "Version", y = "Runtime (s)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

plot_2 <- ggplot(data_2, aes(x = Version, y = Time)) +
  geom_boxplot() +
  labs(title = "H100-96", x = "Version", y = "Runtime (s)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

plot <- plot_1 + plot_2
plot
```

\newpage
## Appendix

### Result Reproduction

#### Input

Input tests can be generated using the `gpu_gen.sh` script where we create 30 random tests with different parameters. Part of the script is shown below for reference. Take note of the composition of the tests.

```bash
# Random no wildcards tests
for i in {1..10}
do
    ./gen_sig 1000 3000 10000 0.0 > tests/sig_${i}.fasta
    ./gen_sample tests/sig_${i}.fasta 2000 20 1 2 100000 200000 10 30 0.0 > tests/samp_${i}.fastq
done

# Random wildcards tests
for i in {11..20}
do
    ./gen_sig 1000 3000 10000 0.1 > tests/sig_${i}.fasta
    ./gen_sample tests/sig_${i}.fasta 2000 20 1 2 100000 200000 10 30 0.1 > tests/samp_${i}.fastq
done

# Extreme min tests (2 tests with no wildcards + 3 tests with wildcards)
for i in {21..22}
do
    ./gen_sig 500 3000 3000 0.0 > tests/sig_${i}.fasta
    ./gen_sample tests/sig_${i}.fasta 980 20 1 2 100000 100000 10 30 0.0 > tests/samp_${i}.fastq
done
for i in {23..25}
do
    ./gen_sig 500 3000 3000 0.1 > tests/sig_${i}.fasta
    ./gen_sample tests/sig_${i}.fasta 980 20 1 2 100000 100000 10 30 0.1 > tests/samp_${i}.fastq
done

# Extreme max tests (2 tests with no wildcards + 3 tests with wildcards)
for i in {26..27}
do
    ./gen_sig 1000 10000 10000 0.0 > tests/sig_${i}.fasta
    ./gen_sample tests/sig_${i}.fasta 2180 20 1 2 200000 200000 10 30 0.0 > tests/samp_${i}.fastq
done
for i in {28..30}
do
    ./gen_sig 1000 10000 10000 0.1 > tests/sig_${i}.fasta
    ./gen_sample tests/sig_${i}.fasta 2180 20 1 2 200000 200000 10 30 0.1 > tests/samp_${i}.fastq
done
```

#### GPU Nodes Used

To test on `a100-40 MIG GPU`, we use node `xgph10`. To test on `h100-96 GPU`, we use node `xgpi0`.

#### Execution Time Measurement

The scripts `gpu_benchmark_{a/h}.sh` and `gpu_job_{a/h}.sh` are used with `sbatch` to run the benchmark and our implementation against all the tests generated above. Since `gpu_job_{a/h}.sh` only compiles the `kernel_skeleton.cu` file, we need to replace it with the version of the kernel that we want to test each time, such as `kernel_without_opt1.cu` and `kernel_without_opt2.cu`.

The overall time measurement of the `runMatcher` function is extracted from the standard error stream of the job output at the line containing `(FOR AUTOMATED CHECKING) Total runMatcher time:`. We extract these measurements through a helper script `extract_time.py` and store them in the CSV files `a100-40.csv` and `h100-96.csv` in the folder `report_data/optimization` for easier analysis. These CSV files are shown below for reference.

```{r}
data_1 <- read_csv("report_data/optimization/a100-40.csv")
data_2 <- read_csv("report_data/optimization/h100-96.csv")
knitr::kable(data_1, caption = "A100-40")
knitr::kable(data_2, caption = "H100-96")
```